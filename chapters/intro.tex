\section{Введение}
Около 10 лет назад история развития компьютерной графики реального времени потерпела кардинальный поворот с выходом графических API нового поколения, DirectX12, Vulkan и Metal, в 2014, 2016 и 2014 годах соответственно.
Их предшественники, OpenGL и DirectX ранних версий, основывались на идее так называемого <<толстого>> драйвера.
Дизайн этих API старался максимально скрыть принципы работы видеокарт, предоставляя пользователям простой, но достаточно ограниченный инструмент для разработки графических приложений реального времени.
По мере развития индустрии компьютерной графики, разработчики приложений всё чаще сталкивались с ограничениями старых API, а простота дизайна всё больше жертвовалась в пользу поддержки новых возможностей графических ускорителей.

Обусловлен этот процесс возрастающими требованиями к приложениям со стороны клиентов.
В индустриях игр, кино, виртуальных тренировочных симуляторов, а также промышленной визуализации, очень важны качество и реализм картинки.
За годы академических исследований компьютерной графики было предложено большое количество техник и алгоритмов визуализации самых различных сцен, а развитие потребительской и промышленной техники привело к распространению широкого спектра устройств для запуска приложений.
Так, не редка ситуация в которой одно и то же приложение должно масштабироваться от профессиональных персональных компьютеров с мощнейшими процессорами, видеокартами и мониторами в разрешении 8K, и до потребительских HMD-устройств, и даже телефонов и портативных консолей.
От разработчиков приложений ожидается масштабируемость на этот широкий спектр устройств, с учётом всей их специфики, при этом добиваясь максимального возможного реализма картинки для конкретного устройства, не забывая так же о бизнес-требованиях к производительности.

Главной целью дизайна графических API нового поколения было позволить разработчикам оправдать эти ожидания, в следствии чего произошёл отказ от <<толстых>> драйверов в пользу раскрытия всё большего числа деталей работы графических ускорителей.
Доступ к низкоуровневым механизмам графических ускорителей позволил приложениям лучше адаптироваться под конкретные устройства, рациональнее использовать доступные ресурсы, и как следствие, достигать большего реализма и производительности.
Однако с большой властью приходит большая ответственность: сложность новых API требует от разработчика сильно более структурированного подхода к разработки приложений, нежели старые, разработки собственных абстракций, собственных алгоритмов управления различными ресурсами.
В следующем подразделе более детально освещены различные аспекты перехода к новым API, а также проблемы, возникающие при разработке приложений с их использованием.

\subsection{Аспекты разработки графических приложений реального времени в старых и новых API}
\subsubsection{Управление памятью транзиентных ресурсов GPU}
В процессе вычисления картинки одного кадра любое нетривиальное приложение использует \textit{транзиентные ресурсы} -- промежуточные хранилища данных, содержимое которых не требуется после окончания вычисления кадра, либо, требуется лишь в процессе вычисления следующего кадра.
Как правило, подобные ресурсы являются картинками с разрешением кратным разрешению монитора пользователя.
Из этого следует, что при переходе от 1080p мониторов к 4K мониторам потребление памяти транзиентными ресурсами возрастает в 4 раза, что обуславливает нужду в эффективном её переиспользовании.
Старые графические API полностью скрывали управление памятью GPU от пользователя, предоставляя лишь функции создания и удаления конкретных ресурсов.
За годы существования этой абстракции образовалось 3 основных подхода к эффективному управлению памятью транзиентных ресурсов.

Самый простым подходом является выделение и освобождение транзиентных ресурсов по ходу их нужды при помощи соответствующих вызовов графического API.
Этот подход фактически идентичен выделению памяти в различных языках программирования: драйвер операционной системы содержит аллокатор, на который пользователь перекладывает обязанность управления памятью и другими ресурсами GPU, аналогично аллокациям на куче в языке C.
Системный аллокатор переиспользует освободившуюся память, тем самым достигая низкого её потребления.
Однако, такой подход не масштабируется на более сложные приложения. Во-первых известны нижние оценки на качество работы аллокаторов реального времени \todo{ссылка на оценку онлайн DSA}, на практике выражающиеся как фрагментация кучи.
Во-вторых, как правило и создание и освобождение ресурсов является весьма дорогой операцией в следствии деталей реализации драйверов.
\todo{мб объяснить почему?}

Альтернативным подходом служит отказ от переиспользования памяти. Все транзиентные ресурсы создаются заранее и не удаляются в ходе работы приложения. Очевидно, что с повышением сложности приложения такой подход перестаёт быть применимым, что иногда влечёт к попыткам в ручную переиспользовать некоторые выделенные объекты. Это, в свою очередь, приводит к чрезвычайно сложному для понимания коду, усложняя работу над самим приложением.

Наконец, наиболее практичным подходом является \textit{пулирование} ресурсов.
Вся программа работает с объектом называемым \textit{пулом}, отвечающим за выделение и освобождение ресурсов.
Пул использует аллокатор драйвера для выделения новых ресурсов, но вместо освобождения ресурсов в драйвер хранит список неиспользуемых ресурсов конкретного типа (в понятие тип как правило входит разрешение для текстур и размер для буфферов соответственно, а также все флаги свойств ресурса).
Последующие запросы на выделение ресурсов обслуживаются в первую очередь из списка неиспользуемых, и только исчерпав его выделяются новые посредством драйвера операционной системы.
Данный подход был оптимален до появления современных низкоуровневых графических API, однако в настоящее время хорошо заметен его главный недостаток: память не переиспользуется между ресурсами разных типов.

В современных же графических API предоставляется прямой, неограниченный доступ к памяти GPU.
Приложение может выделять \textit{кучи}, последовательности страниц виртуальной видеопамяти, и затем создавать ресурсы на конкретных адресах в рамках конкретной кучи.
Стоит отметить, что пересечение используемых разными ресурсами регионов кучи не запрещается, хоть поведение при одновременном использование таких ресурсов не определено.
Фактически, это нововведение перекладывает ответственность по написанию аллокатора ресурсов с разработчиков драйвера на разработчиков приложения, что можно сравнить с разработкой на языке C используя лишь системный вызов \inlcpp{malloc}.
С одной стороны, это сильно усложняет разработку простых приложений.
В следствии этого компания AMD открыла исходный код аллокатора из своих драйверов \todo{ссылка}, к использованию которого нередко прибегают даже в промышленных приложениях, что, конечно же, возвращает статус-кво старых графических API.
С другой стороны, это даёт возможность разработчикам более эффективно распоряжаться видеопамятью в различных подсистемах приложения, в частности, позволяя построить в некотором смысле оптимальное расписание аллокации транзиентных ресурсов.

\subsubsection{Отправка команд GPU}
Работа с любыми внешними по отношению к центральному процессору устройствами по своей природе асинхронна.
Передача данных по проводам занимает время, как и их обработка на внешнем устройстве.
Заставлять ядра центрального процессора простаивать в ожидании отклика от внешнего устройства -- непозволительная растрата ресурсов.
Не исключение и графические ускорители.
Низкоуровневым инструментом для общения GPU и операционной системы служат \textit{списки команд}, состоящии из команд отрисовки, запуска вычислений, синхронизации, копирования данных, и прочих.
В случае если центральному процессору необходимо дождаться результата каких либо вычислений на GPU, ожидание необходимо делать вручную, используя аппаратные сигналы о прогрессе от видеоускорителя.

В старых API асинхронная природа вычислений на GPU скрывалась за абстракцией \textit{мгновенного режима} (от англ. immediate mode).
Драйвер создавал видимость мгновенного выполнения всех команд GPU, представлявших собой функции. Примитивы синхронизации внутри GPU, а также между GPU и CPU вставлялись автоматически, что не редко приводило к непредсказуемой производительности кода. Более того, производительность крупных приложений могла упираться в скорость записи командных буферов внутри драйвера. Естественным способом решения этой проблемы была бы параллельная их запись, но машина состояний внутри драйверов старых графических API была фундаментально однопоточной структурой. Далее, в какой-то момент времени графически ускорители начали поддерживать параллельную обработку нескольких очередей команд. Эта возможность может давать прирост производительности при слабой загрузке вычислительных модулей GPU исполняемыми командами. Дизайн старых API не был рассчитан на поддержку таких возможностей аппаратуры, что сильно усложняло работу с ними.

С приходом новых графических API фактически все проблемы с отправкой команд были решены. Новые API предоставляют пользователю прямой доступ к спискам команд, примитивам синхронизации и очередям исполнения команд. Однако, как и в случае с прямым доступом к памяти, от разработчиков требуется некоторый уровень дисциплины при работе с предоставляемыми абстракциями.
\todo{Как-то оборвано, возможно стоит плавнее перейти к записи команд буферов через фреймграф. Или вообще не говорить про это всё?}

\subsubsection{Управление кешами GPU и состоянием ресурсов}
Характерным отличием графических ускорителей от центральных процессоров является отсутствие гарантий когерентности кешей, и в следствии чего необходимость в ручную делать их инвалидацию и сброс. Однако старые графические API скрывали эту особенность аппаратуры, автоматически отслеживая состояние ресурса и вставляя соответствующие команды синхронизации в список команд автоматически. Недостаток такого подхода заключается в отсутствии невозможности предсказывать последующие команды пользователя на уровне драйвера, что не редко приводит к исполнению команд синхронизации в неоптимальный момент времени. Усугубляет ситуацию тот факт, что некоторые GPU используют различные оптимизации формата хранения ресурсов при их использовании конкретным образом, и переход между разными оптимизированными состояниями приводит к простою вычислительных ресурсов GPU.

В новых же графических API ответственность за отслеживание состояния ресурсов и кешей переложена на пользователя посредством абстракции \textit{барьеров}. Барьеры служат главным примитивом синхронизации в рамках GPU, управления кешами и состояниями ресурсов. Однако расстановка барьеров в корректных и оптимальных местах порой оказывается далеко не тривиальной задачей, требующей от пользователя глобального понимания работы всей системы. Это делает модуляризацию приложения невозможным без введения специальных механизмов для расстановки барьеров.

\subsection{Кадровый граф}
Сложно отследить появление понятия вычислительного графа, однако первым приложением этой техники в контексте графических приложений реального времени считается игровой движок Frostbite компании EA, о чём было объявлено в 2017 году на конференции Game Developers Conference \cite{FrostbiteGdcTalk}.
\todo{Написать про:}
\begin{itemize}
\item \todo{память}
\item \todo{архитектурную крутизну ресурсных зависимостей}
\item \todo{барьеры}
\item \todo{рендерпассы и TBDR}
\item \todo{автоматический подбор хорошего для платформы порядка исполнения}
\item \todo{многопоточную запись команд}
\item \todo{автоматический асинк}
\item \todo{мультиплексирование и прочие "циклы"}
\item \todo{историю ресурсов}
\end{itemize}
