\section{Введение}
Около 10 лет назад история развития компьютерной графики реального времени потерпела кардинальный поворот с выходом графических API нового поколения, DirectX12, Vulkan и Metal, в 2014, 2016 и 2014 годах соответственно.
Их предшественники, OpenGL и DirectX ранних версий, основывались на идее так называемого <<толстого>> драйвера.
Дизайн этих API старался максимально скрыть принципы работы видеокарт, предоставляя пользователям простой, но достаточно ограниченный инструмент для разработки графических приложений реального времени.
По мере развития индустрии компьютерной графики, разработчики приложений всё чаще сталкивались с ограничениями старых API, а простота дизайна всё больше жертвовалась в пользу поддержки новых возможностей графических ускорителей.

Обусловлен этот процесс возрастающими требованиями к приложениям со стороны клиентов.
В индустриях игр, кино, виртуальных тренировочных симуляторов, а также промышленной визуализации, очень важны качество и реализм картинки.
За годы академических исследований компьютерной графики было предложено большое количество техник и алгоритмов визуализации самых различных сцен, а развитие потребительской и промышленной техники привело к распространению широкого спектра устройств для запуска приложений.
Так, не редка ситуация в которой одно и то же приложение должно масштабироваться от профессиональных персональных компьютеров с мощнейшими процессорами, видеокартами и мониторами в разрешении 8K, и до потребительских HMD-устройств, и даже телефонов и портативных консолей.
От разработчиков приложений ожидается масштабируемость на этот широкий спектр устройств, с учётом всей их специфики, при этом добиваясь максимального возможного реализма картинки для конкретного устройства, не забывая так же о бизнес-требованиях к производительности.

Главной целью дизайна графических API нового поколения было позволить разработчикам оправдать эти ожидания, в следствии чего произошёл отказ от <<толстых>> драйверов в пользу раскрытия всё большего числа деталей работы графических ускорителей.
Доступ к низкоуровневым механизмам графических ускорителей позволил приложениям лучше адаптироваться под конкретные устройства, рациональнее использовать доступные ресурсы, и как следствие, достигать большего реализма и производительности.
Однако с большой властью приходит большая ответственность: сложность новых API требует от разработчика сильно более структурированного подхода к разработки приложений, нежели старые, разработки собственных абстракций, собственных алгоритмов управления различными ресурсами.
В следующем подразделе более детально освещены различные аспекты перехода к новым API, а также проблемы, возникающие при разработке приложений с их использованием.

\subsection{Аспекты разработки графических приложений реального времени в старых и новых API}
\subsubsection*{Управление памятью транзиентных ресурсов GPU}
В процессе вычисления картинки одного кадра любое нетривиальное приложение использует \textit{транзиентные ресурсы} -- промежуточные хранилища данных, содержимое которых не требуется после окончания вычисления кадра, либо, требуется лишь в процессе вычисления следующего кадра.
Как правило, подобные ресурсы являются картинками с разрешением кратным разрешению монитора пользователя.
Из этого следует, что при переходе от 1080p мониторов к 4K мониторам потребление памяти транзиентными ресурсами возрастает в 4 раза, что обуславливает нужду в эффективном её переиспользовании.
Старые графические API полностью скрывали управление памятью GPU от пользователя, предоставляя лишь функции создания и удаления конкретных ресурсов.
За годы существования этой абстракции образовалось 3 основных подхода к эффективному управлению памятью транзиентных ресурсов.

Самый простым подходом является выделение и освобождение транзиентных ресурсов по ходу их нужды при помощи соответствующих вызовов графического API.
Этот подход фактически идентичен выделению памяти в различных языках программирования: драйвер операционной системы содержит аллокатор, на который пользователь перекладывает обязанность управления памятью и другими ресурсами GPU, аналогично аллокациям на куче в языке C.
Системный аллокатор переиспользует освободившуюся память, тем самым достигая низкого её потребления.
Однако, такой подход не масштабируется на более сложные приложения. Во-первых известны нижние оценки на качество работы аллокаторов реального времени \todo{ссылка на оценку онлайн DSA}, на практике выражающиеся как фрагментация кучи.
Во-вторых, как правило и создание и освобождение ресурсов является весьма дорогой операцией в следствии деталей реализации драйверов.
\todo{мб объяснить почему?}

Альтернативным подходом служит отказ от переиспользования памяти. Все транзиентные ресурсы создаются заранее и не удаляются в ходе работы приложения. Очевидно, что с повышением сложности приложения такой подход перестаёт быть применимым, что иногда влечёт к попыткам в ручную переиспользовать некоторые выделенные объекты. Это, в свою очередь, приводит к чрезвычайно сложному для понимания коду, усложняя работу над самим приложением.

Наконец, наиболее практичным подходом является \textit{пулирование} ресурсов.
Вся программа работает с объектом называемым \textit{пулом}, отвечающим за выделение и освобождение ресурсов.
Пул использует аллокатор драйвера для выделения новых ресурсов, но вместо освобождения ресурсов в драйвер хранит список неиспользуемых ресурсов конкретного типа (в понятие тип как правило входит разрешение для текстур и размер для буфферов соответственно, а также все флаги свойств ресурса).
Последующие запросы на выделение ресурсов обслуживаются в первую очередь из списка неиспользуемых, и только исчерпав его выделяются новые посредством драйвера операционной системы.
Данный подход был оптимален до появления современных низкоуровневых графических API, однако в настоящее время хорошо заметен его главный недостаток: память не переиспользуется между ресурсами разных типов.

В современных же графических API предоставляется прямой, неограниченный доступ к памяти GPU.
Приложение может выделять \textit{кучи}, последовательности страниц виртуальной видеопамяти, и затем создавать ресурсы на конкретных адресах в рамках конкретной кучи.
Стоит отметить, что пересечение используемых разными ресурсами регионов кучи не запрещается, хоть поведение при одновременном использование таких ресурсов не определено.
Фактически, это нововведение перекладывает ответственность по написанию аллокатора ресурсов с разработчиков драйвера на разработчиков приложения, что можно сравнить с разработкой на языке C используя лишь системный вызов \inlcpp{malloc}.
С одной стороны, это сильно усложняет разработку простых приложений.
В следствии этого компания AMD открыла исходный код аллокатора из своих драйверов \todo{ссылка}, к использованию которого нередко прибегают даже в промышленных приложениях, что, конечно же, возвращает статус-кво старых графических API.
С другой стороны, это даёт возможность разработчикам более эффективно распоряжаться видеопамятью в различных подсистемах приложения, в частности, позволяя построить в некотором смысле оптимальное расписание аллокации транзиентных ресурсов.

\subsubsection*{Отправка команд GPU}
Работа с любыми внешними по отношению к центральному процессору устройствами по своей природе асинхронна.
Передача данных по проводам занимает время, как и их обработка на внешнем устройстве.
Заставлять ядра центрального процессора простаивать в ожидании отклика от внешнего устройства -- непозволительная растрата ресурсов.
Не исключение и графические ускорители.
Низкоуровневым инструментом для общения GPU и операционной системы служат \textit{списки команд}, состоящии из команд отрисовки, запуска вычислений, синхронизации, копирования данных, и прочих.
В случае если центральному процессору необходимо дождаться результата каких либо вычислений на GPU, ожидание необходимо делать вручную, используя аппаратные сигналы о прогрессе от видеоускорителя.

В старых API асинхронная природа вычислений на GPU скрывалась за абстракцией \textit{мгновенного режима} (от англ. immediate mode).
Драйвер создавал видимость мгновенного выполнения всех команд GPU, представлявших собой функции. Примитивы синхронизации внутри GPU, а также между GPU и CPU вставлялись автоматически, что не редко приводило к непредсказуемой производительности кода. Более того, производительность крупных приложений могла упираться в скорость записи командных буферов внутри драйвера. Естественным способом решения этой проблемы была бы параллельная их запись, но машина состояний внутри драйверов старых графических API была фундаментально однопоточной структурой. Далее, в какой-то момент времени графически ускорители начали поддерживать параллельную обработку нескольких очередей команд. Эта возможность может давать прирост производительности при слабой загрузке вычислительных модулей GPU исполняемыми командами. Дизайн старых API не был рассчитан на поддержку таких возможностей аппаратуры, что сильно усложняло работу с ними.

С приходом новых графических API фактически все проблемы с отправкой команд были решены. Новые API предоставляют пользователю прямой доступ к спискам команд, примитивам синхронизации и очередям исполнения команд. Однако, как и в случае с прямым доступом к памяти, от разработчиков требуется некоторый уровень дисциплины при работе с предоставляемыми абстракциями.
\todo{Как-то оборвано, возможно стоит плавнее перейти к записи команд буферов через фреймграф. Или вообще не говорить про это всё?}

\subsubsection*{Управление кешами GPU и состоянием ресурсов}
Характерным отличием графических ускорителей от центральных процессоров является отсутствие гарантий когерентности кешей, и в следствии чего необходимость в ручную делать их инвалидацию и сброс. Однако старые графические API скрывали эту особенность аппаратуры, автоматически отслеживая состояние ресурса и вставляя соответствующие команды синхронизации в список команд автоматически. Недостаток такого подхода заключается в отсутствии невозможности предсказывать последующие команды пользователя на уровне драйвера, что не редко приводит к исполнению команд синхронизации в неоптимальный момент времени. Усугубляет ситуацию тот факт, что некоторые GPU используют различные оптимизации формата хранения ресурсов при их использовании конкретным образом, и переход между разными оптимизированными состояниями приводит к простою вычислительных ресурсов GPU.

В новых же графических API ответственность за отслеживание состояния ресурсов и кешей переложена на пользователя посредством абстракции \textit{барьеров}. Барьеры служат главным примитивом синхронизации в рамках GPU, управления кешами и состояниями ресурсов. Однако расстановка барьеров в корректных и оптимальных местах порой оказывается далеко не тривиальной задачей, требующей от пользователя глобального понимания работы всей системы. Это делает модуляризацию приложения невозможным без введения специальных механизмов для расстановки барьеров.

\subsection{Кадровый граф}
Сложно отследить появление понятия вычислительного графа, однако первым приложением этой техники в контексте графических приложений реального времени считается игровой движок Frostbite компании EA, о чём было объявлено в 2017 году на конференции Game Developers Conference \cite{FrostbiteGdcTalk}.
С тех пор большая часть коммерческих движков перешла на архитектуру основанную на кадровом графе, о чём подробнее в следующем разделе.
Причина этого перехода -- наличие кадрового графа в виде данных позволяет решить все вышеописанные проблемы, появившиеся с приходом новых графических API, при этом сделать это оптимальнее чем позволяли встроенные в драйвер механизмы старых API.

Уточним используемую в дальнейшем терминологию.
\textit{Кадровым графом} назовём конкретный набор вершин, рёбер и других данных, определяемый настройками и спецификой приложения, и задающий глобальную структуру процесса вычисления одного кадра.
\textit{Рантаймом} кадровых графов будем называть программное решение, принимающее на вход конкретный кадровый граф и позволяющее его \textit{компилировать}, запускать и, возможно, редактировать.
Процесс компиляции кадрового графа -- некоторый набор действий и вычислений, которые необходимо совершить перед запуском кадрового графа.
Конкретный набор данных, задающий кадровый граф, определяется дизайном конкретного рантайма, как и специфика процессов компиляции, запуска, а также дополнительный функционал, предоставляемый пользователям рантайма, а именно, разработчикам алгоритмов визуализации сцены.
Пространство дизайна рантайма кадровых графов весьма широко, но независимо от конкретных решений, рантайм будет принимать на вход набор вершин, каждая из которых содержит \textit{функцию запуска}, инициирующую некоторые вычисления на GPU, и список используемых функцией запуска ресурсов с дополнительной информацией о конкретном способе использования.
Рёбра же графа как правило не задаются явно, а создаются автоматически, например, между вершиной создающей некоторый ресурс, и читающей этот ресурс вершиной.
Абстрагируясь от конкретики, опишем как именно архитектура, основанная на кадровом графе, позволяет решить описанные выше проблемы.

Во-первых, имея в виде данных глобальную информацию об использовании ресурсов на протяжении всего кадра, вопрос управления памятью GPU сводится к неинтерактивной вариации хорошо известной в литературе задачи динамической аллокации памяти \cite[с. 226]{10.5555/574848}.
В отсутствии глобальной информации управление памятью является интерактивной вариацией задачи о динамической аллокации памяти.
Известно, что алгоритмы для интерактивной вариации этой задачи работают качественно хуже, чем для статической.
В программной инженерии этот результат известен как проблема фрагментации кучи.
Более того, как уже было упомянуто выше, создание ресурсов GPU -- достаточно дорогая операция, а качественные алгоритмы решения интерактивной динамической аллокации памяти занимают значительное время.
Эти факторы делают решение неинтерактивной вариации этой задачи один раз в момент компиляции кадрового графа привлекательной идеей.

Во-вторых, если рантайм требует от пользователей указывать конкретный способ использования ресурсов внутри вершины, то в процессе запуска кадрового графа становится возможным автоматически расставлять барьеры.
Если ресурс был создан и заполнен данными посредством рендеринга вершиной $A$, а затем после запуска некоторого количества других вершин был семплирован вершиной $B$, то рантайм обязан поставить барьер переводящий ресурс из состояния пригодного для рендеринга в состояние пригодное для семплирования.
Однако из-за наличия нескольких промежуточных вершин, у рантайма есть выбор, в какой конкретно момент поставить барьер.
Этот выбор может влиять на производительность из-за конвейеризации вычислений на современных графических ускорителях, а значит можно сформулировать задачу дискретной оптимизации расстановки барьеров с целью минимизации простоев вычислительных модулей GPU.
В данной работе, однако, такая задача не рассматривается.

В-третьих, интересной особенностью современных графических API, не упомянутой ранее, является поддержка специфичной для мобильных устройств архитектуры графических ускорителей, называемой <<Tile Based Deferred Renderer>> (TBDR).
Не вдаваясь в детяли принципов работы TBDR \todo{может всё таки вдаться?}, старые графические API были абсолютно не рассчитаны на подобные графические ускорители, что приводило к непредсказуемому влиянию изменений в коде на производительность, а также делало некоторые техники визуализации вроде отложенного освещения неприменимыми на подобных устройствах.
В новых же API был предоставлен почти полный контроль над механизмами работы TBDR, что с одной стороны решило эти проблемы, но с другой стороны сильно усложнило процесс разработки из-за введения понятия \textit{рендер-пасса} \cite[раздел~8]{VulkanSpec}.
Введения слоя абстракции над графическим API в виде кадрового графа позволяет одновременно и упростить интерфейс предоставляемый пользователю для работы с TBDR и рендер-пассами, и сохранить предсказуемость производительности, отчасти путём предоставления инструментов визуализации структуры графа.

В-четвёртых, как уже было упомянуто выше, современные GPU часто предоставляют несколько очередей для отправки команд, что позволяет лучше насытить вычислительные модули в ситуации когда выполняемые команды не требуют стопроцентного использования всех возможностей ускорителя. Рантайм кадровых графов может помочь автоматизировать этот процесс, заранее выбирая стратегию планирования имеющихся вершин на имеющиеся очереди команд. Далее, даже в рамках одной очереди команд не редко имеет смысл чередовать команды от независимых ветвей графа с целью лучше насытить вычислительные ресурсы GPU полезной работой. Более того, само исполнение вершин кадрового графа для получения списков команд рантайм может производить параллельно для независимых ветвей графа, что может понизить время затрачиваемое на кадр на центральном процессоре. В данной работе, однако, более глубоко данный вопрос не освещается.

Наконец, пожалуй самым важным фактором в распространении кадровых графов послужили архитектурные преимущества этого подхода. Как было сказано ранее, рёбра графа обычно задаются пользователями рантайма неявно, посредством ресурсных зависимостей. Это позволяет добиться низкой связности различных подсистем приложения: различным модулям не требуется знать о прочей системе ничего, кроме названий ресурсов и публичного API рантайма кадровых графов. Более того, рантайм может автоматически обнаруживать некорректные конфигурации приложения, в которых тем или иным модулям не хватает необходимых ресурсных зависимостей, отключая соответствующие модули автоматически.

Конечно же, далеко не все имплементации рантаймов кадровых графов решают все перечисленные проблемы, и не всегда описанным выше образом. Некоторые же имплементации содержат уникальный для них функционал, решающий специфичные для конкретной области проблемы. В следующем разделе приведён краткий обзор существующих решений.
